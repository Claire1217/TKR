{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tkr_generator import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class channel1(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(channel1, self).__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(1, 96, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.relu1 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv2 = nn.Conv2d(96, 10, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(10, 4, kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.fc1 = nn.Linear(4096, 120)\n",
    "\t\tself.fc2 = nn.Linear(120, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.pool(self.relu1(x))\n",
    "\t\t\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = self.pool(self.relu1(x))\n",
    "\n",
    "\t\tx = self.conv3(x)\n",
    "\t\tx = self.pool(self.relu1(x))\n",
    "\n",
    "\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channel1(\n",
       "  (conv1): Conv2d(1, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(96, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(10, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=4096, out_features=120, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc2): Linear(in_features=120, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = channel1()\n",
    "model.load_state_dict(torch.load('tmp_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TKRDataset('R:\\OAIKNEE-Q1126\\OAI_UQ_processed\\AKOA_TKR_Data', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_loader.__getitem__(0)\n",
    "images = torch.tensor(images, dtype=float)\n",
    "outputs = model(images.reshape(4,1,256,256).float())\n",
    "pred = torch.round(torch.sigmoid(outputs)).detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1852\\2913905685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not bool"
     ]
    }
   ],
   "source": [
    "torch.sum(labels == pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(100):\n",
    "    images, labels = train_loader.__getitem__(0)\n",
    "    images = torch.tensor(images, dtype=float)\n",
    "    outputs = model(images.reshape(4,1,256,256).float())\n",
    "    pred = torch.round(torch.sigmoid(outputs)).detach().numpy().squeeze()\n",
    "    count += np.sum(pred == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    21] loss: 0.605\n",
      "[1,    41] loss: 0.610\n",
      "[1,    61] loss: 0.633\n",
      "[1,    81] loss: 0.599\n",
      "[1,   101] loss: 0.663\n",
      "[1,   121] loss: 0.601\n",
      "[1,   141] loss: 0.548\n",
      "[1,   161] loss: 0.660\n",
      "[1,   181] loss: 0.577\n",
      "[1,   201] loss: 0.592\n",
      "[1,   221] loss: 0.598\n",
      "[1,   241] loss: 0.518\n",
      "[1,   261] loss: 0.665\n",
      "[1,   281] loss: 0.637\n",
      "[1,   301] loss: 0.557\n",
      "[1,   321] loss: 0.698\n",
      "[1,   341] loss: 0.656\n",
      "[1,   361] loss: 0.597\n",
      "[1,   381] loss: 0.654\n",
      "[1,   401] loss: 0.605\n",
      "[1,   421] loss: 0.702\n",
      "[1,   441] loss: 0.574\n",
      "[1,   461] loss: 0.543\n",
      "[1,   481] loss: 0.651\n",
      "[1,   501] loss: 0.582\n",
      "[1,   521] loss: 0.629\n",
      "[1,   541] loss: 0.536\n",
      "[1,   561] loss: 0.663\n",
      "[1,   581] loss: 0.595\n",
      "[1,   601] loss: 0.622\n",
      "[1,   621] loss: 0.634\n",
      "[1,   641] loss: 0.620\n",
      "[1,   661] loss: 0.620\n",
      "[1,   681] loss: 0.561\n",
      "[1,   701] loss: 0.562\n",
      "[1,   721] loss: 0.638\n",
      "[1,   741] loss: 0.590\n",
      "[1,   761] loss: 0.552\n",
      "[1,   781] loss: 0.591\n",
      "[1,   801] loss: 0.583\n",
      "[1,   821] loss: 0.534\n",
      "[1,   841] loss: 0.569\n",
      "[1,   861] loss: 0.642\n",
      "[1,   881] loss: 0.567\n",
      "[1,   901] loss: 0.544\n",
      "[1,   921] loss: 0.632\n",
      "[1,   941] loss: 0.588\n",
      "[1,   961] loss: 0.485\n",
      "[1,   981] loss: 0.605\n",
      "[1,  1001] loss: 0.493\n",
      "[1,  1021] loss: 0.666\n"
     ]
    }
   ],
   "source": [
    "# Param = {\n",
    "#     'LR': 0.0001,\n",
    "#     'WC': 0.0001\n",
    "# }\n",
    "# num_epochs = 1\n",
    "# model_type = 'densenet'\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(\"The model will be running on\", device, \"device\")\n",
    "# # Convert model parameters and buffers to CPU or Cuda\n",
    "# model = channel1().float()\n",
    "# loss_fn = nn.BCELoss()\n",
    "# optimizer = Adam(model.parameters(), lr=Param['LR'], weight_decay=Param['WC'])\n",
    "# best_accuracy = 0.0\n",
    "\n",
    "# print(os.getcwd())\n",
    "# train_loader = TKRDataset('R:\\OAIKNEE-Q1126\\OAI_UQ_processed\\AKOA_TKR_Data', 0)\n",
    "\n",
    "\n",
    "# Define your execution device\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        images = torch.tensor(images, dtype=float).to(device, dtype=float)\n",
    "        labels = torch.tensor(labels, dtype=float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # predict classes using images from the training set\n",
    "        outputs = model(images.reshape(4,1,256,256).float())\n",
    "        pred = torch.sigmoid(outputs)\n",
    "        \n",
    "        # compute the loss based on model output and real labels\n",
    "        loss = loss_fn(pred[:,0], labels.float())\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "        # adjust parameters based on the calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Let's print statistics for every 1,000 images\n",
    "        running_loss += loss.item()     # extract the loss value\n",
    "        \n",
    "        if i % 20 == 0 and not i== 0:    \n",
    "            # print every 1000 (twice per epoch) \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "            # zero the loss\n",
    "            if running_loss / 20 < 0.5:\n",
    "                los = running_loss /20\n",
    "                torch.save(model.state_dict(), f'sagittal_weights_iter{i}_loss{los}.pth')\n",
    "            running_loss = 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    21] loss: 0.564\n",
      "[1,    41] loss: 0.577\n",
      "[1,    61] loss: 0.600\n",
      "[1,    81] loss: 0.673\n",
      "[1,   101] loss: 0.569\n",
      "[1,   121] loss: 0.594\n",
      "[1,   141] loss: 0.613\n",
      "[1,   161] loss: 0.680\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        images = torch.tensor(images, dtype=float).to(device, dtype=float)\n",
    "        labels = torch.tensor(labels, dtype=float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # predict classes using images from the training set\n",
    "        outputs = model(images.reshape(4,1,256,256).float())\n",
    "        pred = torch.sigmoid(outputs)\n",
    "        \n",
    "        # compute the loss based on model output and real labels\n",
    "        loss = loss_fn(pred[:,0], labels.float())\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "        # adjust parameters based on the calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Let's print statistics for every 1,000 images\n",
    "        running_loss += loss.item()     # extract the loss value\n",
    "        \n",
    "        if i % 20 == 0 and not i== 0:    \n",
    "            # print every 1000 (twice per epoch) \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "            # zero the loss\n",
    "            if running_loss / 20 < 0.5:\n",
    "                los = running_loss /20\n",
    "                torch.save(model.state_dict(), f'sagittal_weights_iter{i}_loss{los}.pth')\n",
    "            running_loss = 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    21] loss: 0.630\n",
      "[1,    41] loss: 0.605\n",
      "[1,    61] loss: 0.520\n",
      "[1,    81] loss: 0.549\n",
      "[1,   101] loss: 0.494\n",
      "[1,   121] loss: 0.548\n",
      "[1,   141] loss: 0.466\n",
      "[1,   161] loss: 0.568\n",
      "[1,   181] loss: 0.645\n",
      "[1,   201] loss: 0.573\n",
      "[1,   221] loss: 0.580\n",
      "[1,   241] loss: 0.579\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        images = torch.tensor(images, dtype=float).to(device, dtype=float)\n",
    "        labels = torch.tensor(labels, dtype=float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # predict classes using images from the training set\n",
    "        outputs = model(images.reshape(4,1,256,256).float())\n",
    "        pred = torch.sigmoid(outputs)\n",
    "        \n",
    "        # compute the loss based on model output and real labels\n",
    "        loss = loss_fn(pred[:,0], labels.float())\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "        # adjust parameters based on the calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Let's print statistics for every 1,000 images\n",
    "        running_loss += loss.item()     # extract the loss value\n",
    "        \n",
    "        if i % 20 == 0 and not i== 0:    \n",
    "            # print every 1000 (twice per epoch) \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "            # zero the loss\n",
    "            if running_loss / 20 < 0.5:\n",
    "                los = running_loss /20\n",
    "                torch.save(model.state_dict(), f'sagittal_weights_iter{i}_loss{los}.pth')\n",
    "            running_loss = 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(100):\n",
    "    images, labels = train_loader.__getitem__(0)\n",
    "    images = torch.tensor(images, dtype=float)\n",
    "    outputs = model(images.reshape(4,1,256,256).float())\n",
    "    pred = torch.round(torch.sigmoid(outputs)).detach().numpy().squeeze()\n",
    "    count += np.sum(pred == labels)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        images = torch.tensor(images, dtype=float).to(device, dtype=float)\n",
    "        labels = torch.tensor(labels, dtype=float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # predict classes using images from the training set\n",
    "        outputs = model(images.reshape(4,1,256,256).float())\n",
    "        pred = torch.sigmoid(outputs)\n",
    "        \n",
    "        # compute the loss based on model output and real labels\n",
    "        loss = loss_fn(pred[:,0], labels.float())\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "        # adjust parameters based on the calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Let's print statistics for every 1,000 images\n",
    "        running_loss += loss.item()     # extract the loss value\n",
    "        \n",
    "        if i % 20 == 0 and not i== 0:    \n",
    "            # print every 1000 (twice per epoch) \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "            # zero the loss\n",
    "            if running_loss / 20 < 0.5:\n",
    "                los = running_loss /20\n",
    "                torch.save(model.state_dict(), f'sagittal_weights_iter{i}_loss{los}.pth')\n",
    "            running_loss = 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    21] loss: 0.594\n",
      "[1,    41] loss: 0.524\n",
      "[1,    61] loss: 0.599\n",
      "[1,    81] loss: 0.580\n",
      "[1,   101] loss: 0.551\n",
      "[1,   121] loss: 0.576\n",
      "[1,   141] loss: 0.598\n",
      "[1,   161] loss: 0.584\n",
      "[1,   181] loss: 0.518\n",
      "[1,   201] loss: 0.493\n",
      "[1,   221] loss: 0.525\n",
      "[1,   241] loss: 0.683\n",
      "[1,   261] loss: 0.634\n",
      "[1,   281] loss: 0.558\n",
      "[1,   301] loss: 0.644\n",
      "[1,   321] loss: 0.519\n",
      "[1,   341] loss: 0.631\n",
      "[1,   361] loss: 0.584\n",
      "[1,   381] loss: 0.613\n",
      "[1,   401] loss: 0.544\n",
      "[1,   421] loss: 0.544\n",
      "[1,   441] loss: 0.611\n",
      "[1,   461] loss: 0.610\n",
      "[1,   481] loss: 0.566\n",
      "[1,   501] loss: 0.626\n",
      "[1,   521] loss: 0.520\n",
      "[1,   541] loss: 0.583\n",
      "[1,   561] loss: 0.577\n",
      "[1,   581] loss: 0.562\n",
      "[1,   601] loss: 0.526\n",
      "[1,   621] loss: 0.567\n",
      "[1,   641] loss: 0.661\n",
      "[1,   661] loss: 0.615\n",
      "[1,   681] loss: 0.583\n",
      "[1,   701] loss: 0.570\n",
      "[1,   721] loss: 0.601\n",
      "[1,   741] loss: 0.514\n",
      "[1,   761] loss: 0.441\n",
      "[1,   781] loss: 0.546\n",
      "[1,   801] loss: 0.523\n",
      "[1,   821] loss: 0.495\n",
      "[1,   841] loss: 0.521\n",
      "[1,   861] loss: 0.693\n",
      "[1,   881] loss: 0.591\n",
      "[1,   901] loss: 0.506\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        images = torch.tensor(images, dtype=float).to(device, dtype=float)\n",
    "        labels = torch.tensor(labels, dtype=float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # predict classes using images from the training set\n",
    "        outputs = model(images.reshape(4,1,256,256).float())\n",
    "        pred = torch.sigmoid(outputs)\n",
    "        \n",
    "        # compute the loss based on model output and real labels\n",
    "        loss = loss_fn(pred[:,0], labels.float())\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "        # adjust parameters based on the calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Let's print statistics for every 1,000 images\n",
    "        running_loss += loss.item()     # extract the loss value\n",
    "        \n",
    "        if i % 20 == 0 and not i== 0:    \n",
    "            # print every 1000 (twice per epoch) \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "            # zero the loss\n",
    "            if running_loss / 20 < 0.5:\n",
    "                los = running_loss /20\n",
    "                torch.save(model.state_dict(), f'sagittal_weights_iter{i}_loss{los}.pth')\n",
    "            running_loss = 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    21] loss: 0.539\n",
      "[1,    41] loss: 0.564\n",
      "[1,    61] loss: 0.568\n",
      "[1,    81] loss: 0.754\n",
      "[1,   101] loss: 0.645\n",
      "[1,   121] loss: 0.579\n",
      "[1,   141] loss: 0.615\n",
      "[1,   161] loss: 0.500\n",
      "[1,   181] loss: 0.597\n",
      "[1,   201] loss: 0.490\n",
      "[1,   221] loss: 0.474\n",
      "[1,   241] loss: 0.625\n",
      "[1,   261] loss: 0.513\n",
      "[1,   281] loss: 0.509\n",
      "[1,   301] loss: 0.626\n",
      "[1,   321] loss: 0.504\n",
      "[1,   341] loss: 0.478\n",
      "[1,   361] loss: 0.582\n",
      "[1,   381] loss: 0.560\n",
      "[1,   401] loss: 0.531\n",
      "[1,   421] loss: 0.591\n",
      "[1,   441] loss: 0.493\n",
      "[1,   461] loss: 0.570\n",
      "[1,   481] loss: 0.586\n",
      "[1,   501] loss: 0.546\n",
      "[1,   521] loss: 0.569\n",
      "[1,   541] loss: 0.549\n",
      "[1,   561] loss: 0.515\n",
      "[1,   581] loss: 0.609\n",
      "[1,   601] loss: 0.645\n",
      "[1,   621] loss: 0.497\n",
      "[1,   641] loss: 0.466\n",
      "[1,   661] loss: 0.459\n",
      "[1,   681] loss: 0.591\n",
      "[1,   701] loss: 0.587\n",
      "[1,   721] loss: 0.482\n",
      "[1,   741] loss: 0.648\n",
      "[1,   761] loss: 0.554\n",
      "[1,   781] loss: 0.635\n",
      "[1,   801] loss: 0.590\n",
      "[1,   821] loss: 0.454\n",
      "[1,   841] loss: 0.541\n",
      "[1,   861] loss: 0.572\n",
      "[1,   881] loss: 0.561\n",
      "[1,   901] loss: 0.577\n",
      "[1,   921] loss: 0.476\n",
      "[1,   941] loss: 0.667\n",
      "[1,   961] loss: 0.562\n",
      "[1,   981] loss: 0.490\n",
      "[1,  1001] loss: 0.540\n",
      "[1,  1021] loss: 0.587\n",
      "[1,  1041] loss: 0.653\n",
      "[1,  1061] loss: 0.550\n",
      "[1,  1081] loss: 0.550\n",
      "[1,  1101] loss: 0.591\n",
      "[1,  1121] loss: 0.575\n",
      "[1,  1141] loss: 0.567\n",
      "[1,  1161] loss: 0.559\n",
      "[1,  1181] loss: 0.586\n",
      "[1,  1201] loss: 0.547\n",
      "[1,  1221] loss: 0.601\n",
      "[1,  1241] loss: 0.607\n",
      "[1,  1261] loss: 0.633\n",
      "[1,  1281] loss: 0.552\n",
      "[1,  1301] loss: 0.576\n",
      "[1,  1321] loss: 0.488\n",
      "[1,  1341] loss: 0.564\n",
      "[1,  1361] loss: 0.571\n",
      "[1,  1381] loss: 0.513\n",
      "[1,  1401] loss: 0.516\n",
      "[1,  1421] loss: 0.518\n",
      "[1,  1441] loss: 0.538\n",
      "[1,  1461] loss: 0.587\n",
      "[1,  1481] loss: 0.571\n",
      "[1,  1501] loss: 0.623\n",
      "[1,  1521] loss: 0.593\n",
      "[1,  1541] loss: 0.618\n",
      "[1,  1561] loss: 0.649\n",
      "[1,  1581] loss: 0.638\n",
      "[1,  1601] loss: 0.586\n",
      "[1,  1621] loss: 0.560\n",
      "[1,  1641] loss: 0.586\n",
      "[1,  1661] loss: 0.554\n",
      "[1,  1681] loss: 0.647\n",
      "[1,  1701] loss: 0.560\n",
      "[1,  1721] loss: 0.585\n",
      "[1,  1741] loss: 0.626\n",
      "[1,  1761] loss: 0.597\n",
      "[1,  1781] loss: 0.551\n",
      "[1,  1801] loss: 0.539\n",
      "[1,  1821] loss: 0.511\n",
      "[1,  1841] loss: 0.481\n",
      "[1,  1861] loss: 0.647\n",
      "[1,  1881] loss: 0.510\n",
      "[1,  1901] loss: 0.664\n",
      "[1,  1921] loss: 0.610\n",
      "[1,  1941] loss: 0.515\n",
      "[1,  1961] loss: 0.520\n",
      "[1,  1981] loss: 0.530\n",
      "[1,  2001] loss: 0.609\n",
      "[1,  2021] loss: 0.535\n",
      "[1,  2041] loss: 0.512\n",
      "[1,  2061] loss: 0.609\n",
      "[1,  2081] loss: 0.551\n",
      "[1,  2101] loss: 0.538\n",
      "[1,  2121] loss: 0.607\n",
      "[1,  2141] loss: 0.608\n",
      "[1,  2161] loss: 0.506\n",
      "[1,  2181] loss: 0.567\n",
      "[1,  2201] loss: 0.549\n",
      "[1,  2221] loss: 0.489\n",
      "[1,  2241] loss: 0.506\n",
      "[1,  2261] loss: 0.647\n",
      "[1,  2281] loss: 0.572\n",
      "[1,  2301] loss: 0.544\n",
      "[1,  2321] loss: 0.499\n",
      "[1,  2341] loss: 0.542\n",
      "[1,  2361] loss: 0.495\n",
      "[1,  2381] loss: 0.583\n",
      "[1,  2401] loss: 0.427\n",
      "[1,  2421] loss: 0.514\n",
      "[1,  2441] loss: 0.516\n",
      "[1,  2461] loss: 0.541\n",
      "[1,  2481] loss: 0.557\n",
      "[1,  2501] loss: 0.587\n",
      "[1,  2521] loss: 0.588\n",
      "[1,  2541] loss: 0.510\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        images = torch.tensor(images, dtype=float).to(device, dtype=float)\n",
    "        labels = torch.tensor(labels, dtype=float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # predict classes using images from the training set\n",
    "        outputs = model(images.reshape(4,1,256,256).float())\n",
    "        pred = torch.sigmoid(outputs)\n",
    "        \n",
    "        # compute the loss based on model output and real labels\n",
    "        loss = loss_fn(pred[:,0], labels.float())\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "        # adjust parameters based on the calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Let's print statistics for every 1,000 images\n",
    "        running_loss += loss.item()     # extract the loss value\n",
    "        \n",
    "        if i % 20 == 0 and not i== 0:    \n",
    "            # print every 1000 (twice per epoch) \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "            # zero the loss\n",
    "            if running_loss / 20 < 0.5:\n",
    "                los = running_loss /20\n",
    "                torch.save(model.state_dict(), f'sagittal_weights_iter{i}_loss{los}.pth')\n",
    "            running_loss = 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f80f8f2c86ad4cfab96b7b4f8fdfd1b4cf2cfcb15b76a3f739b49f4c7d4ee0d4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
